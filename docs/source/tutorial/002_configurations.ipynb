{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nAdvanced Configurations\n=======================\n\nDefining Parameter Spaces\n-------------------------\n\nOptuna supports five kinds of parameters.\n\n.. code-block:: python\n\n    def objective(trial):\n        # Categorical parameter\n        optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])\n\n        # Int parameter\n        num_layers = trial.suggest_int('num_layers', 1, 3)\n\n        # Uniform parameter\n        dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)\n\n        # Loguniform parameter\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n\n        # Discrete-uniform parameter\n        drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1)\n\n        ...\n\nBranches and Loops\n------------------\n\nYou can use branches or loops depending on the parameter values.\n\n.. code-block:: python\n\n    def objective(trial):\n        classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n        if classifier_name == 'SVC':\n            svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n            classifier_obj = sklearn.svm.SVC(C=svc_c)\n        else:\n            rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n            classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth)\n\n        ...\n\n.. code-block:: python\n\n    def create_model(trial):\n        n_layers = trial.suggest_int('n_layers', 1, 3)\n\n        layers = []\n        for i in range(n_layers):\n            n_units = int(trial.suggest_loguniform('n_units_l{}'.format(i), 4, 128))\n            layers.append(L.Linear(None, n_units))\n            layers.append(F.relu)\n        layers.append(L.Linear(None, 10))\n\n        return chainer.Sequential(*layers)\n\nPlease also refer to `examples <https://github.com/optuna/optuna/tree/master/examples>`_.\n\n\nNote on the Number of Parameters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe difficulty of optimization increases roughly exponentially with regard to the number of parameters. That is, the number of necessary trials increases exponentially when you increase the number of parameters, so it is recommended to not add unimportant parameters.\n\n\nArguments for `Study.optimize`\n--------------------------------\n\nThe method :func:`~optuna.study.Study.optimize` (and ``optuna study optimize`` CLI command as well)\nhas several useful options such as ``timeout``.\nFor details, please refer to the API reference for :func:`~optuna.study.Study.optimize`.\n\n**FYI**: If you give neither ``n_trials`` nor ``timeout`` options, the optimization continues until it receives a termination signal such as Ctrl+C or SIGTERM.\nThis is useful for use cases such as when it is hard to estimate the computational costs required to optimize your objective function.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}